{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2533359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# NTU-60 skeleton parent list (used for bone vectors)\n",
    "PARENTS = [0, 0, 1, 2,\n",
    "           1, 4, 5, 6,\n",
    "           1, 8, 9,10,\n",
    "           0,12,13,14,\n",
    "           0,16,17,18,\n",
    "           1, 7, 7,11,11]\n",
    "\n",
    "def extract_basic(data):\n",
    "    \"\"\"Mean+Std over time, joints, persons → (N, 6)\"\"\"\n",
    "    N,C,T,V,M = data.shape\n",
    "    m = data.mean(axis=(2,3,4))\n",
    "    s = data.std(axis=(2,3,4))\n",
    "    return np.concatenate([m, s], axis=1)\n",
    "\n",
    "def extract_rich(data):\n",
    "    \"\"\"Adds velocity, acceleration, and bone-vector stats → (N, 48)\"\"\"\n",
    "    N,C,T,V,M = data.shape\n",
    "    X = data.mean(axis=4)     # (N,C,T,V)\n",
    "\n",
    "    vel = np.diff(X, axis=2)      # (N,C,T−1,V)\n",
    "    acc = np.diff(vel, axis=2)    # (N,C,T−2,V)\n",
    "\n",
    "    # bone vectors\n",
    "    bone = np.zeros_like(X)\n",
    "    for j in range(1, V):\n",
    "        bone[..., j] = X[..., j] - X[..., PARENTS[j]]\n",
    "\n",
    "    def pool(Y):\n",
    "        m = Y.mean(axis=(2,3))\n",
    "        s = Y.std(axis=(2,3))\n",
    "        return np.concatenate([m, s], axis=1)\n",
    "\n",
    "    return np.concatenate([pool(X), pool(vel), pool(acc), pool(bone)], axis=1)\n",
    "\n",
    "def extract_extended(data, joint_pairs):\n",
    "    \"\"\"\n",
    "    In addition to extract_rich, compute for each (i,j) in joint_pairs:\n",
    "    mean+std of Euclid-dist(frame,i−j) → 2*len(joint_pairs) dims\n",
    "    \"\"\"\n",
    "    N,C,T,V,M = data.shape\n",
    "    X = data.mean(axis=4)  # (N,C,T,V)\n",
    "    d_feats = []\n",
    "    for (i,j) in joint_pairs:\n",
    "        # per-frame distance\n",
    "        d = np.linalg.norm(X[:,:,:,i] - X[:,:,:,j], axis=1)  # (N,T)\n",
    "        d_feats.append(d.mean(axis=1))\n",
    "        d_feats.append(d.std (axis=1))\n",
    "    d_feats = np.stack(d_feats, axis=1)  # (N, 2*len(pairs))\n",
    "\n",
    "    return np.concatenate([ extract_rich(data),\n",
    "                            d_feats ], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "319dbae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# adjust these paths\n",
    "DATA_PATH  = r'D:\\UniGe\\2\\CV\\Babel Project\\Normalized Data\\train_ntu_sk_60_pre.npy'\n",
    "LABEL_PATH = r'D:\\UniGe\\2\\CV\\Babel Project\\BABEL\\action_recognition\\data\\release\\train_label_60.pkl'\n",
    "\n",
    "data = np.load(DATA_PATH)         # (45473, 3, 150, 25, 1)\n",
    "with open(LABEL_PATH,'rb') as f:\n",
    "    labels = pickle.load(f)[1][0]  # flat array of shape (45473,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "096a5df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Prepare feature sets:\n",
    "X_basic    = extract_basic   (data)\n",
    "X_rich     = extract_rich    (data)\n",
    "# pick some pairs (i,j) manually, e.g. head (3)→neck (2), wrist(6)→elbow(5)...\n",
    "pairs      = [(3,2),(6,5),(9,8),(12,10),(15,13)]\n",
    "X_ext      = extract_extended(data, pairs)\n",
    "\n",
    "# 2) Train/test split\n",
    "Xb_tr, Xb_te, y_tr, y_te = train_test_split(X_basic, labels, test_size=0.2, random_state=42)\n",
    "Xr_tr, Xr_te            = train_test_split(X_rich,  labels, test_size=0.2, random_state=42)[0:2]\n",
    "Xe_tr, Xe_te            = train_test_split(X_ext,   labels, test_size=0.2, random_state=42)[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8fb8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Define models\n",
    "models = [\n",
    "  ('LogReg',   LogisticRegression(max_iter=1000, class_weight='balanced')),\n",
    "  ('KNN',      KNeighborsClassifier(n_neighbors=5)),\n",
    "  ('RandomF',  RandomForestClassifier(n_estimators=200, class_weight='balanced')),\n",
    "  ('GBoost',   GradientBoostingClassifier(n_estimators=100)),\n",
    "  ('SVM',      SVC(kernel='rbf', C=1.0, class_weight='balanced'))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12169ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg   | Basic → acc: 0.174\n",
      "LogReg   | Rich  → acc: 0.234\n",
      "LogReg   | Ext   → acc: 0.247\n",
      "----------------------------------------\n",
      "KNN      | Basic → acc: 0.291\n",
      "KNN      | Rich  → acc: 0.328\n",
      "KNN      | Ext   → acc: 0.344\n",
      "----------------------------------------\n",
      "RandomF  | Basic → acc: 0.269\n",
      "RandomF  | Rich  → acc: 0.306\n",
      "RandomF  | Ext   → acc: 0.326\n",
      "----------------------------------------\n",
      "GBoost   | Basic → acc: 0.284\n",
      "GBoost   | Rich  → acc: 0.326\n",
      "GBoost   | Ext   → acc: 0.342\n",
      "----------------------------------------\n",
      "SVM      | Basic → acc: 0.248\n",
      "SVM      | Rich  → acc: 0.282\n",
      "SVM      | Ext   → acc: 0.293\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 4) Evaluate each on Basic→Rich→Ext\n",
    "for name, clf in models:\n",
    "    for X_tr, X_te, tag in [(Xb_tr,Xb_te,'Basic'),\n",
    "                            (Xr_tr,Xr_te,'Rich'),\n",
    "                            (Xe_tr,Xe_te,'Ext')]:\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        p = clf.predict(X_te)\n",
    "        acc = accuracy_score(y_te, p)\n",
    "        print(f\"{name:8s} | {tag:5s} → acc: {acc:.3f}\")\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30606b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition      import PCA\n",
    "from sklearn.neighbors         import KNeighborsClassifier\n",
    "from sklearn.model_selection   import GridSearchCV\n",
    "\n",
    "# 1) PCA to 30 dims\n",
    "pca = PCA(n_components=30, random_state=42)\n",
    "Xp = pca.fit_transform(X_ext)\n",
    "\n",
    "# 2) Grid-search k & weights\n",
    "param_grid = {\n",
    "    'n_neighbors': [3,5,7,9],\n",
    "    'weights':    ['uniform','distance'],\n",
    "    'metric':     ['euclidean','cosine']\n",
    "}\n",
    "knn = KNeighborsClassifier()\n",
    "gs  = GridSearchCV(knn, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "gs.fit(Xp, labels)\n",
    "\n",
    "print(\"Best params:\", gs.best_params_)\n",
    "print(\"CV score:  \", gs.best_score_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
